{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae81a901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\3737639145.py:19: FutureWarning: content is deprecated, use rawContent instead\n",
      "  else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\3737639145.py:28: FutureWarning: content is deprecated, use rawContent instead\n",
      "  else: tonyhawk_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "shaq_twitter_handle = 'SHAQ'\n",
    "tonyhawk_twitter_handle = 'tonyhawk'\n",
    "start = '2022-01-01'\n",
    "end = '2023-01-28'\n",
    "tonyhawk_query = \"(from:{}) until:{} since:{}\".format(tonyhawk_twitter_handle, end, start)\n",
    "shaq_query = \"(from:{}) until:{} since:{}\".format(shaq_twitter_handle, end, start)\n",
    "shaq_tweet_arr = []\n",
    "tonyhawk_tweet_arr = []\n",
    "\n",
    "limit = 100\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(shaq_query).get_items():\n",
    "    \n",
    "    if len(shaq_tweet_arr) == limit:\n",
    "        break\n",
    "    else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "shaq_tweets = pd.DataFrame(shaq_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "shaq_tweets.to_csv('tweets.csv', mode = 'a')\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(tonyhawk_query).get_items():\n",
    "    \n",
    "    if len(tonyhawk_tweet_arr) == limit:\n",
    "        break\n",
    "    else: tonyhawk_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "        \n",
    "tonyhawk_tweets = pd.DataFrame(tonyhawk_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "tonyhawk_tweets.to_csv('tweets.csv', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee56822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "##get_100_Tweets(twitter_handle) definition\n",
    "\n",
    "\n",
    "def get_100_tweets(twitter_handle):\n",
    "\tstart = '2019-01-01'  #YYYY-MM-DD   (just starting far back enough to ensure there are 100)\n",
    "\tend = '2023-02-03'  #YYYY-MM-DD\n",
    "\n",
    "\tquery = \"(from:{}) until:{} since:{}\".format(twitter_handle, end, start)\n",
    "\ttweets = []\n",
    "\tlimit = 100\n",
    "\n",
    "\tfor tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "\t\tif len(tweets) == limit:\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\ttweets.append([\n",
    "\t\t\t tweet.url, tweet.date, tweet.rawContent, tweet.id, tweet.user.username,\n",
    "\t\t\t tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount,\n",
    "\t\t\t tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates\n",
    "\t\t\t])\n",
    "\treturn tweets\n",
    "\n",
    "\n",
    "##Scraping 100 tweets from @ddockett and @Danica Patrick\n",
    "##-----------------------------------------\n",
    "\n",
    "twitter_handles = ['ddockett', 'DanicaPatrick']\n",
    "all_tweets = []\n",
    "\n",
    "for handle in twitter_handles:\n",
    "\tall_tweets += get_100_tweets(handle)\n",
    "\n",
    "reggie_dwyane_tweets = pd.DataFrame(all_tweets,\n",
    "                                    columns=[\n",
    "                                     'url', 'date', 'content', 'id', 'user',\n",
    "                                     'replyCount', 'retweetCount', 'likeCount',\n",
    "                                     'qouteCount', 'conversationId', 'lang',\n",
    "                                     'source', 'coordinates'\n",
    "                                    ])\n",
    "reggie_dwyane_tweets.to_csv('tweets.csv', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0685faf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13808\\2580305202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtweetList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTwitterSearchScraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetList\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'snt' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"(from:{}) until:{} since:{}\".format('JHarden13', '2023-02-01' , '2022-2-01' )\n",
    "tweetList = []\n",
    "\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweetList) == 100:\n",
    "        break\n",
    "    else:\n",
    "         tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "HardenTweet = pd.DataFrame(tweetList, columns=['Url','Date','Content','ID','User','Reply Count','Retweet Count','Like Count','Quote Count','Conversation ID','Language','Source','Coordinates'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef41f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\1691920078.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "\n",
    "query = \"(from:{}) until:{} since:{}\".format('lancearmstrong', '2023-02-01' , '2022-2-01' )\n",
    "tweetList = []\n",
    "\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweetList) == 100:\n",
    "        break\n",
    "    else:\n",
    "         tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "ArmstrongTweet = pd.DataFrame(tweetList, columns=['Url','Date','Content','ID','User','Reply Count','Retweet Count','Like Count','Quote Count','Conversation ID','Language','Source','Coordinates'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012dd9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\4014320768.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "\n",
    "query = \"(from:{}) until:{} since:{}\".format('JHarden13', '2023-02-01' , '2022-2-01' )\n",
    "tweetList = []\n",
    "\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweetList) == 100:\n",
    "        break\n",
    "    else:\n",
    "         tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "HardenTweet = pd.DataFrame(tweetList, columns=['Url','Date','Content','ID','User','Reply Count','Retweet Count','Like Count','Quote Count','Conversation ID','Language','Source','Coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9e0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "HardenTweet.to_csv('tweets.csv',index=False,mode='a')\n",
    "ArmstrongTweet.to_csv('tweets.csv',index=False,mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97a541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "##get_100_Tweets(twitter_handle) definition\n",
    "\n",
    "def get_100_tweets(twitter_handle):\n",
    "    start = '2019-01-01' #YYYY-MM-DD   (just starting far back enough to ensure there are 100)\n",
    "    end = '2023-02-03' #YYYY-MM-DD\n",
    "\n",
    "    query = \"(from:{}) until:{} since:{}\".format(twitter_handle, end, start)\n",
    "    tweets = []\n",
    "    limit = 100\n",
    "\n",
    "\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "        if len(tweets) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.url,\n",
    "                           tweet.date,\n",
    "                           tweet.rawContent,\n",
    "                           tweet.id,\n",
    "                           tweet.user.username,\n",
    "                           tweet.replyCount,\n",
    "                           tweet.retweetCount,\n",
    "                           tweet.likeCount,\n",
    "                           tweet.quoteCount,\n",
    "                           tweet.conversationId,\n",
    "                           tweet.lang,\n",
    "                           tweet.source,\n",
    "                           tweet.coordinates])\n",
    "    return tweets\n",
    "\n",
    "\n",
    "##Scraping 100 tweets from @ReggieBush and @DwyaneWade\n",
    "##-----------------------------------------\n",
    "\n",
    "twitter_handles = ['ReggieBush','DwyaneWade']\n",
    "all_tweets = []\n",
    "\n",
    "for handle in twitter_handles:\n",
    "    all_tweets += get_100_tweets(handle)\n",
    "\n",
    "\n",
    "reggie_dwyane_tweets = pd.DataFrame(all_tweets, columns=['url', 'date', 'content', 'id', 'user', 'replyCount', 'retweetCount', 'likeCount', 'qouteCount', 'conversationId', 'lang', 'source', 'coordinates'])\n",
    "reggie_dwyane_tweets.to_csv('tweets.csv', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ef82f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported table #0\n",
      "Exported table #1\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13808\\2993618520.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mdf_master\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtwitter_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtwitter_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mdf_master\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"tweets.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3549\u001b[0m         )\n\u001b[0;32m   3550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3551\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         )\n\u001b[1;32m-> 1180\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'tweets.csv'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "#Users to scrape from\n",
    "twitter_handles = [\"strombone1\", \"cesc4official\"]\n",
    "#YYYY-MM-DD\n",
    "start = \"2016-01-01\"\n",
    "end = \"2023-02-02\"\n",
    "\n",
    "#Initialize dictionary to hold user keys and dataframes values\n",
    "all_data = {}\n",
    "#Loop through \n",
    "for handle in twitter_handles:\n",
    "    query = \"(from:{}) until:{} since:{}\".format(handle, end, start)\n",
    "    tweets = []\n",
    "    limit = 100\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        if len(tweets) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.url,\n",
    "                            tweet.date,\n",
    "                            tweet.rawContent,\n",
    "                            tweet.id,\n",
    "                            tweet.user,\n",
    "                            tweet.replyCount,\n",
    "                            tweet.retweetCount,\n",
    "                            tweet.likeCount,\n",
    "                            tweet.quoteCount,\n",
    "                            tweet.conversationId,\n",
    "                            tweet.lang,\n",
    "                            tweet.source,\n",
    "                            tweet.coordinates\n",
    "                            ])\n",
    "    collected_tweets_df = pd.DataFrame(tweets, columns=['Url',\n",
    "                                                        'Date',\n",
    "                                                        'Content',\n",
    "                                                        'Id',\n",
    "                                                        'User',\n",
    "                                                        'Reply Count',\n",
    "                                                        'Retweet Count',\n",
    "                                                        'Like Count',\n",
    "                                                        'Quote Count',\n",
    "                                                        'Conversation Id',\n",
    "                                                        'Language',\n",
    "                                                        'Source',\n",
    "                                                        'Coordinates'\n",
    "                                                        ])\n",
    "    all_data[handle] = collected_tweets_df\n",
    "    \n",
    "for index, handle in enumerate(all_data.keys()):\n",
    "    #print(all_data[handle])\n",
    "    #print(\"======================\")\n",
    "    all_data[handle].to_csv(path_or_buf=f\"tweets_{handle}.csv\")\n",
    "    print(f\"Exported table #{index}\")\n",
    "\n",
    "all_data[\"strombone1\"].head(10)\n",
    "\n",
    "df_master = pd.concat([all_data[twitter_handles[0]], all_data[twitter_handles[1]]], ignore_index=True)\n",
    "df_master.to_csv(path_or_buf=f\"tweets.csv\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9261d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported table #0\n",
      "Exported table #1\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "#Users to scrape from\n",
    "twitter_handles = [\"strombone1\", \"cesc4official\"]\n",
    "#YYYY-MM-DD\n",
    "start = \"2016-01-01\"\n",
    "end = \"2023-02-02\"\n",
    "\n",
    "#Initialize dictionary to hold user keys and dataframes values\n",
    "all_data = {}\n",
    "#Loop through \n",
    "for handle in twitter_handles:\n",
    "    query = \"(from:{}) until:{} since:{}\".format(handle, end, start)\n",
    "    tweets = []\n",
    "    limit = 100\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        if len(tweets) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.url,\n",
    "                            tweet.date,\n",
    "                            tweet.rawContent,\n",
    "                            tweet.id,\n",
    "                            tweet.user,\n",
    "                            tweet.replyCount,\n",
    "                            tweet.retweetCount,\n",
    "                            tweet.likeCount,\n",
    "                            tweet.quoteCount,\n",
    "                            tweet.conversationId,\n",
    "                            tweet.lang,\n",
    "                            tweet.source,\n",
    "                            tweet.coordinates\n",
    "                            ])\n",
    "    collected_tweets_df = pd.DataFrame(tweets, columns=['Url',\n",
    "                                                        'Date',\n",
    "                                                        'Content',\n",
    "                                                        'Id',\n",
    "                                                        'User',\n",
    "                                                        'Reply Count',\n",
    "                                                        'Retweet Count',\n",
    "                                                        'Like Count',\n",
    "                                                        'Quote Count',\n",
    "                                                        'Conversation Id',\n",
    "                                                        'Language',\n",
    "                                                        'Source',\n",
    "                                                        'Coordinates'\n",
    "                                                        ])\n",
    "    all_data[handle] = collected_tweets_df\n",
    "    \n",
    "for index, handle in enumerate(all_data.keys()):\n",
    "    #print(all_data[handle])\n",
    "    #print(\"======================\")\n",
    "    all_data[handle].to_csv(path_or_buf=f\"tweets_{handle}.csv\")\n",
    "    print(f\"Exported table #{index}\")\n",
    "\n",
    "all_data[\"strombone1\"].head(10)\n",
    "\n",
    "df_master = pd.concat([all_data[twitter_handles[0]], all_data[twitter_handles[1]]], ignore_index=True)\n",
    "df_master.to_csv(path_or_buf=f\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dddd75af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported table #0\n",
      "Exported table #1\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "#Users to scrape from\n",
    "twitter_handles = [\"strombone1\", \"cesc4official\"]\n",
    "#YYYY-MM-DD\n",
    "start = \"2016-01-01\"\n",
    "end = \"2023-02-02\"\n",
    "\n",
    "#Initialize dictionary to hold user keys and dataframes values\n",
    "all_data = {}\n",
    "#Loop through \n",
    "for handle in twitter_handles:\n",
    "    query = \"(from:{}) until:{} since:{}\".format(handle, end, start)\n",
    "    tweets = []\n",
    "    limit = 100\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        if len(tweets) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.url,\n",
    "                            tweet.date,\n",
    "                            tweet.rawContent,\n",
    "                            tweet.id,\n",
    "                            tweet.user,\n",
    "                            tweet.replyCount,\n",
    "                            tweet.retweetCount,\n",
    "                            tweet.likeCount,\n",
    "                            tweet.quoteCount,\n",
    "                            tweet.conversationId,\n",
    "                            tweet.lang,\n",
    "                            tweet.source,\n",
    "                            tweet.coordinates\n",
    "                            ])\n",
    "    collected_tweets_df = pd.DataFrame(tweets, columns=['Url',\n",
    "                                                        'Date',\n",
    "                                                        'Content',\n",
    "                                                        'Id',\n",
    "                                                        'User',\n",
    "                                                        'Reply Count',\n",
    "                                                        'Retweet Count',\n",
    "                                                        'Like Count',\n",
    "                                                        'Quote Count',\n",
    "                                                        'Conversation Id',\n",
    "                                                        'Language',\n",
    "                                                        'Source',\n",
    "                                                        'Coordinates'\n",
    "                                                        ])\n",
    "    all_data[handle] = collected_tweets_df\n",
    "    \n",
    "for index, handle in enumerate(all_data.keys()):\n",
    "    #print(all_data[handle])\n",
    "    #print(\"======================\")\n",
    "    all_data[handle].to_csv(path_or_buf=f\"tweets_{handle}.csv\")\n",
    "    print(f\"Exported table #{index}\")\n",
    "\n",
    "all_data[\"strombone1\"].head(10)\n",
    "\n",
    "df_master = pd.concat([all_data[twitter_handles[0]], all_data[twitter_handles[1]]], ignore_index=True)\n",
    "df_master.to_csv(path_or_buf=f\"tweets.csv\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ecb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "##get_100_Tweets(twitter_handle) definition\n",
    "\n",
    "def get_100_tweets(twitter_handle):\n",
    "    start = '2019-01-01' #YYYY-MM-DD   (just starting far back enough to ensure there are 100)\n",
    "    end = '2023-02-03' #YYYY-MM-DD\n",
    "\n",
    "    query = \"(from:{}) until:{} since:{}\".format(twitter_handle, end, start)\n",
    "    tweets = []\n",
    "    limit = 100\n",
    "\n",
    "\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "        if len(tweets) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.url,\n",
    "                           tweet.date,\n",
    "                           tweet.rawContent,\n",
    "                           tweet.id,\n",
    "                           tweet.user.username,\n",
    "                           tweet.replyCount,\n",
    "                           tweet.retweetCount,\n",
    "                           tweet.likeCount,\n",
    "                           tweet.quoteCount,\n",
    "                           tweet.conversationId,\n",
    "                           tweet.lang,\n",
    "                           tweet.source,\n",
    "                           tweet.coordinates])\n",
    "    return tweets\n",
    "\n",
    "\n",
    "##Scraping 100 tweets from @ReggieBush and @DwyaneWade\n",
    "##-----------------------------------------\n",
    "\n",
    "twitter_handles = ['ReggieBush','DwyaneWade']\n",
    "all_tweets = []\n",
    "\n",
    "for handle in twitter_handles:\n",
    "    all_tweets += get_100_tweets(handle)\n",
    "\n",
    "\n",
    "reggie_dwyane_tweets = pd.DataFrame(all_tweets, columns=['url', 'date', 'content', 'id', 'user', 'replyCount', 'retweetCount', 'likeCount', 'qouteCount', 'conversationId', 'lang', 'source', 'coordinates'])\n",
    "reggie_dwyane_tweets.to_csv('tweets.csv', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e94d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\1691920078.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "\n",
    "query = \"(from:{}) until:{} since:{}\".format('lancearmstrong', '2023-02-01' , '2022-2-01' )\n",
    "tweetList = []\n",
    "\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweetList) == 100:\n",
    "        break\n",
    "    else:\n",
    "         tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "ArmstrongTweet = pd.DataFrame(tweetList, columns=['Url','Date','Content','ID','User','Reply Count','Retweet Count','Like Count','Quote Count','Conversation ID','Language','Source','Coordinates'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "995fdb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\4014320768.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "\n",
    "query = \"(from:{}) until:{} since:{}\".format('JHarden13', '2023-02-01' , '2022-2-01' )\n",
    "tweetList = []\n",
    "\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweetList) == 100:\n",
    "        break\n",
    "    else:\n",
    "         tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "HardenTweet = pd.DataFrame(tweetList, columns=['Url','Date','Content','ID','User','Reply Count','Retweet Count','Like Count','Quote Count','Conversation ID','Language','Source','Coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12c96664",
   "metadata": {},
   "outputs": [],
   "source": [
    "HardenTweet.to_csv('tweets.csv',mode='a')\n",
    "ArmstrongTweet.to_csv('tweets.csv',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42771827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\3737639145.py:19: FutureWarning: content is deprecated, use rawContent instead\n",
      "  else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13808\\3737639145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mshaq_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshaq_tweet_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'URL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Content'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'User'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Replies'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Retweets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Likes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Quotes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Conversation ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Language'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Source'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Coordinates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mshaq_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweets.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msntwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTwitterSearchScraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtonyhawk_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3549\u001b[0m         )\n\u001b[0;32m   3550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3551\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         )\n\u001b[1;32m-> 1180\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'tweets.csv'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "shaq_twitter_handle = 'SHAQ'\n",
    "tonyhawk_twitter_handle = 'tonyhawk'\n",
    "start = '2022-01-01'\n",
    "end = '2023-01-28'\n",
    "tonyhawk_query = \"(from:{}) until:{} since:{}\".format(tonyhawk_twitter_handle, end, start)\n",
    "shaq_query = \"(from:{}) until:{} since:{}\".format(shaq_twitter_handle, end, start)\n",
    "shaq_tweet_arr = []\n",
    "tonyhawk_tweet_arr = []\n",
    "\n",
    "limit = 100\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(shaq_query).get_items():\n",
    "    \n",
    "    if len(shaq_tweet_arr) == limit:\n",
    "        break\n",
    "    else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "shaq_tweets = pd.DataFrame(shaq_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "shaq_tweets.to_csv('tweets.csv', mode = 'a')\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(tonyhawk_query).get_items():\n",
    "    \n",
    "    if len(tonyhawk_tweet_arr) == limit:\n",
    "        break\n",
    "    else: tonyhawk_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "        \n",
    "tonyhawk_tweets = pd.DataFrame(tonyhawk_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "tonyhawk_tweets.to_csv('tweets.csv', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e227ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\3737639145.py:19: FutureWarning: content is deprecated, use rawContent instead\n",
      "  else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\3737639145.py:28: FutureWarning: content is deprecated, use rawContent instead\n",
      "  else: tonyhawk_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "shaq_twitter_handle = 'SHAQ'\n",
    "tonyhawk_twitter_handle = 'tonyhawk'\n",
    "start = '2022-01-01'\n",
    "end = '2023-01-28'\n",
    "tonyhawk_query = \"(from:{}) until:{} since:{}\".format(tonyhawk_twitter_handle, end, start)\n",
    "shaq_query = \"(from:{}) until:{} since:{}\".format(shaq_twitter_handle, end, start)\n",
    "shaq_tweet_arr = []\n",
    "tonyhawk_tweet_arr = []\n",
    "\n",
    "limit = 100\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(shaq_query).get_items():\n",
    "    \n",
    "    if len(shaq_tweet_arr) == limit:\n",
    "        break\n",
    "    else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "shaq_tweets = pd.DataFrame(shaq_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "shaq_tweets.to_csv('tweets.csv', mode = 'a')\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(tonyhawk_query).get_items():\n",
    "    \n",
    "    if len(tonyhawk_tweet_arr) == limit:\n",
    "        break\n",
    "    else: tonyhawk_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "        \n",
    "tonyhawk_tweets = pd.DataFrame(tonyhawk_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "tonyhawk_tweets.to_csv('tweets.csv', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c973e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "##get_100_Tweets(twitter_handle) definition\n",
    "\n",
    "\n",
    "def get_100_tweets(twitter_handle):\n",
    "\tstart = '2019-01-01'  #YYYY-MM-DD   (just starting far back enough to ensure there are 100)\n",
    "\tend = '2023-02-03'  #YYYY-MM-DD\n",
    "\n",
    "\tquery = \"(from:{}) until:{} since:{}\".format(twitter_handle, end, start)\n",
    "\ttweets = []\n",
    "\tlimit = 100\n",
    "\n",
    "\tfor tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "\t\tif len(tweets) == limit:\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\ttweets.append([\n",
    "\t\t\t tweet.url, tweet.date, tweet.rawContent, tweet.id, tweet.user.username,\n",
    "\t\t\t tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount,\n",
    "\t\t\t tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates\n",
    "\t\t\t])\n",
    "\treturn tweets\n",
    "\n",
    "\n",
    "##Scraping 100 tweets from @ddockett and @Danica Patrick\n",
    "##-----------------------------------------\n",
    "\n",
    "twitter_handles = ['ddockett', 'DanicaPatrick']\n",
    "all_tweets = []\n",
    "\n",
    "for handle in twitter_handles:\n",
    "\tall_tweets += get_100_tweets(handle)\n",
    "\n",
    "reggie_dwyane_tweets = pd.DataFrame(all_tweets,\n",
    "                                    columns=[\n",
    "                                     'url', 'date', 'content', 'id', 'user',\n",
    "                                     'replyCount', 'retweetCount', 'likeCount',\n",
    "                                     'qouteCount', 'conversationId', 'lang',\n",
    "                                     'source', 'coordinates'\n",
    "                                    ])\n",
    "reggie_dwyane_tweets.to_csv('tweets.csv', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7811ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\416228363.py:19: FutureWarning: content is deprecated, use rawContent instead\n",
      "  else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_13808\\416228363.py:28: FutureWarning: content is deprecated, use rawContent instead\n",
      "  else: tonyhawk_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "shaq_twitter_handle = 'ddocket'\n",
    "tonyhawk_twitter_handle = 'DanicaPatrick'\n",
    "start = '2022-01-01'\n",
    "end = '2023-01-28'\n",
    "tonyhawk_query = \"(from:{}) until:{} since:{}\".format(tonyhawk_twitter_handle, end, start)\n",
    "shaq_query = \"(from:{}) until:{} since:{}\".format(shaq_twitter_handle, end, start)\n",
    "shaq_tweet_arr = []\n",
    "tonyhawk_tweet_arr = []\n",
    "\n",
    "limit = 100\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(shaq_query).get_items():\n",
    "    \n",
    "    if len(shaq_tweet_arr) == limit:\n",
    "        break\n",
    "    else: shaq_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "shaq_tweets = pd.DataFrame(shaq_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "shaq_tweets.to_csv('tweets.csv', mode = 'a')\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(tonyhawk_query).get_items():\n",
    "    \n",
    "    if len(tonyhawk_tweet_arr) == limit:\n",
    "        break\n",
    "    else: tonyhawk_tweet_arr.append([tweet.url, tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "        \n",
    "tonyhawk_tweets = pd.DataFrame(tonyhawk_tweet_arr, columns=['URL', 'Date', 'Content', 'ID', 'User', 'Replies', 'Retweets', 'Likes', 'Quotes', 'Conversation ID', 'Language', 'Source', 'Coordinates'])\n",
    "tonyhawk_tweets.to_csv('tweets.csv', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465cd68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_10984\\692231141.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "\n",
    "query = \"(from:{}) until:{} since:{}\".format('AngryJoeShow', '2023-02-01' , '2022-2-01' )\n",
    "tweetList = []\n",
    "\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweetList) == 100:\n",
    "        break\n",
    "    else:\n",
    "         tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "AJSTweet = pd.DataFrame(tweetList, columns=['Url','Date','Content','ID','User','Reply Count','Retweet Count','Like Count','Quote Count','Conversation ID','Language','Source','Coordinates'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecdd18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tavien\\AppData\\Local\\Temp\\ipykernel_10984\\4100542253.py:12: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "\n",
    "query = \"(from:{}) until:{} since:{}\".format('Asmongold', '2023-02-01' , '2022-2-01' )\n",
    "tweetList = []\n",
    "\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweetList) == 100:\n",
    "        break\n",
    "    else:\n",
    "         tweetList.append([tweet.url,tweet.date, tweet.content, tweet.id, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.coordinates])\n",
    "\n",
    "AsmonTweet = pd.DataFrame(tweetList, columns=['Url','Date','Content','ID','User','Reply Count','Retweet Count','Like Count','Quote Count','Conversation ID','Language','Source','Coordinates'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ef999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AJSTweet.to_csv('tweets.csv',mode='a')\n",
    "AsmonTweet.to_csv('tweets.csv',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b472cc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported table #0\n",
      "Exported table #1\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "# run variable setup\n",
    "\n",
    "#Users to scrape from\n",
    "twitter_handles = [\"DaraTorres\", \"RafaelNadal\"]\n",
    "\n",
    "#target dates as YYYY-MM-DD; make sure there is a large enough gap to get the listed number of tweets\n",
    "start_time = \"2016-01-01\"\n",
    "end_time = \"2023-02-03\"\n",
    "\n",
    "# use this to set the maximum number of tweets to scrape\n",
    "limit = 100\n",
    "\n",
    "#Initialize dictionary to hold user keys and dataframes values\n",
    "all_data = {}\n",
    "\n",
    "#once variables are setup, start pulling data\n",
    "\n",
    "# loop through target handles\n",
    "for target_handle in twitter_handles:\n",
    "\n",
    "    # setup query for handle\n",
    "    target_query = \"(from:{}) until:{} since:{}\".format(target_handle, end_time, start_time)\n",
    "    tweets_array = []\n",
    "\n",
    "    # query the handle\n",
    "    for tweet in sntwitter.TwitterSearchScraper(target_query).get_items():\n",
    "\n",
    "        # if at limit stop, otherwise record tweet data\n",
    "        if len(tweets_array) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets_array.append([tweet.url,\n",
    "                            tweet.date,\n",
    "                            tweet.rawContent,\n",
    "                            tweet.id,\n",
    "                            tweet.user,\n",
    "                            tweet.replyCount,\n",
    "                            tweet.retweetCount,\n",
    "                            tweet.likeCount,\n",
    "                            tweet.quoteCount,\n",
    "                            tweet.conversationId,\n",
    "                            tweet.lang,\n",
    "                            tweet.source,\n",
    "                            tweet.coordinates\n",
    "                            ])\n",
    "            \n",
    "    # frame the data for export\n",
    "    data_framed_tweets = pd.DataFrame(tweets_array, columns=['Url',\n",
    "                                                        'Date',\n",
    "                                                        'Content',\n",
    "                                                        'Id',\n",
    "                                                        'User',\n",
    "                                                        'Reply Count',\n",
    "                                                        'Retweet Count',\n",
    "                                                        'Like Count',\n",
    "                                                        'Quote Count',\n",
    "                                                        'Conversation Id',\n",
    "                                                        'Language',\n",
    "                                                        'Source',\n",
    "                                                        'Coordinates'\n",
    "                                                        ])\n",
    "\n",
    "    # push to storage while the rest of the data is pulled\n",
    "    all_data[target_handle] = data_framed_tweets\n",
    "\n",
    "# convert pulled data to CSV and export\n",
    "\n",
    "# export each targeted handle by itself\n",
    "for index, target_handle in enumerate(all_data.keys()):\n",
    "    all_data[target_handle].to_csv(path_or_buf=f\"tweets_{target_handle}.csv\")\n",
    "    print(f\"Exported table #{index}\")\n",
    "\n",
    "# export total as well in one larger set\n",
    "df_master = pd.concat([all_data[twitter_handles[0]], all_data[twitter_handles[1]]], ignore_index=True)\n",
    "df_master.to_csv(path_or_buf=f\"tweets.csv\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4b72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
